\chapter{Introduction}

\section{Motivation}
Many scientific and business computing applications must work on large data sets,  
often structured as regular, multidimensional collections.
In order for these applications to achieve good performance (that is, fast execution time), 
programmers must often exploit any potential concurrency through different approaches to parallel programming.
One such approach frequently used with large collections of data is \textit{data parallelism}, 
which is when programs update elements or subsections of collections in parallel.
Tools already exist to help programmers exploit data parallelism 
easily and safely (i.e., with reduced risk of race conditions or program crashes)\cite{dph}\cite{openmp}. 
However, there are two reasons that many of these tools may not be as helpful 
for exploiting data parallelism on regular collections as programmers might like, 
making it more difficult for them to write programs that achieve good performance.

First, many programming languages implement regular collections as nested collections;
e.g. a 2-dimensional, 2 by 3 array (or rank 2 array with shape 2 3) of integers would be implemented as 
an array of 2 arrays, with each of these containing 3 integers. 
However, such languages also often implement irregular collections as nested collections, 
with each sub-collection containing possibly different numbers of elements.
Since these languages implement both regular and irregular collections in the same way,
it can be difficult for programmers to view, manipulate, and verify 
any important properties of the structure of regular collections.
For example, if a function requires that its argument is a regular collection, 
programmers must either write code to ensure this requirement is met,
increasing the difficulty of writing what could be an already complex program, 
or risk run-time indexing errors and program crashes.

Alternatively, regular collections can be implemented as vectors that have an associated shape vector. 
These shape-associated collections make the structure of a regular collection 
a field that programmers can view and manipulate, giving programmers greater transparency and control.
Unfortunately, most statically-typed languages do not have type systems advanced enough 
to statically capture the entire structure of shape-associated collections, 
even though this is theoretically possible. % TODO cite dependent types (xi probably)
However, some languages are able to statically capture some of this structure, 
enabling programmers to reduce or eliminate manual verification, 
such as whether a function's argument is in fact a regular collection,
or that some collection satisfies a function's lower and/or upper bounds for rank\cite{boost}\cite{sac}\cite{dph}.
Even when properties of regular collections cannot be captured statically, 
programming languages and libraries that make the structure of regular collections available at run time 
allow programmers to more easily verify that required properties hold, 
reducing programmer effort and increasing program safety.

Second, in most imperative languages with regular collections, 
applying a function to elements or sub-collections of a collection means 
the function call must be nested within loops, typically \textit{for loop}s.
Using this method to apply functions on elements or sub-collections means that 
the number of dimensions (also called the \textit{rank}) of the collection 
determines the number of nested loops required.
To put it another way, if an existing function that operates on regular collections 
must be extended to a collection of higher rank, 
or if the function operates on scalar values but must be extended to a regular collection, 
the programmer must nest the function in additional for loops.
Nesting for loops adds overhead to the programming process 
and is prone to error because programmers must write boilerplate code.
However, without abstractions in the language or a library 
to deal with the general cases of such extensions, programmers have no other choice.
Moreover, in some cases these extensions are inherently data parallel can be done automatically.
However, nested for loops obfuscate,
and require that the programmer make changes to existing code to exploit, this inherent concurrency. 
Clearly it would be better for programmers to express the potential concurrency directly, 
and thus exploit potential concurrency directly, 
rather than as a sequential for loop, either hoping a compiler will infer and exploit the concurrency 
or writing additional code to do so explicitly.

Functional (in contrast to imperative) programming languages 
allow programmers to write programs at a higher level of abstraction.
When dealing with collections, 
this higher level of abstraction usually means that the similarities of related operations 
are expressed as \textit{higher-order} functions, like \textit{map} and \textit{reduce}, 
that take functions as their arguments and return functions that operate on collections.
These higher-order functions allow programmers to more easily understand the operations done to regular collections 
than if the same operations were done imperatively using for loops 
because they capture the essence of what the operations \textit{are}, more than how the operations are \textit{implemented}.
As a consequence, they also allow programmers 
to see more easily any inherent data parallelism in the algorithm, 
such as all calls to \textit{map}, or calls to \textit{reduce} with associative functions, on large collections. 
The level of abstraction they provide, 
as well as their lack of side-effects (and thus most race conditions), 
make functional languages ideal for solving many kinds of parallel problems.
However, since the higher order functions most functional languages provide for operating on collections 
usually operate on only one dimension at a time, 
they too must be nested to extend existing functions to collections of higher rank, 
with the same, though usually reduced, overhead and error-prone boilerplate code mentioned above.

\textit{Function rank}, first introduced by K. Iverson in 1978\cite{opandfunc} 
and later implemented in the programming language J, 
is a functional abstraction that extends the notion of rank, which usually applies to data only, to functions. 
Like other functional abstractions, 
function rank allows the programmer to directly express 
the application of a function on specific ranks of regular collections 
through a higher-ordered function, called in J and in this paper the \textit{rank operator}\cite{jvocab}.
Expressing these applications as a higher-order function 
makes it easier for a programmer to extend functions to higher ranks safely, quickly, and at a higher level of abstraction.
In some cases, these extensions are so trivial that they can be done automatically, 
meaning the programmer need not modify the code at all\cite{jvocab}\cite{rankanduni}. 
Furthermore, since applying the rank operator multiple times 
is equivalent to the inherently data parallel cases of nesting loops or nesting higher-order functions, 
it too is inherently data parallel.
Consequently, languages with both formalized function rank and a rank-operator 
allow the programmer to exploit the inherent data parallelism in extending functions to collections of higher rank 
safely, quickly, and in some cases automatically.

However, currently the languages that meet these criteria, 
such as J, Sharp APL, and some others from the APL language family, 
are not in common use. 
One reason for their unpopularity is that these languages are difficult to read for most programmers % TODO cite
because, in order to use them effectively, programmers must memorize 
dozens of 1 or 2 character functions each with different, sometimes unrelated use cases\cite{jvocab}\cite{dapl}. 
The design choices of the APL language family, however, 
are not required in order for a language to support formalized function rank.
The field of parallel computing needs a proof of concept that demonstrates 
that formalized function rank and shape-associated collections 
are still very helpful for exploiting data parallelism on regular collections 
when available in a more modern and popularly supported language.

The rest of the paper is organized as follows:
\begin{itemize} 
	\item The remaining sections of this chapter give the design plan (\ref{desp}) and implementation (\ref{imp}) of the research. 
	Unfortunately, the latter did not quite fulfill the full scope of the former, 
	so both a description of what was planned and what was accomplished within the time constraints are given.
	\item Chapter \ref{back} gives the necessary background for understanding this work, 
	including a brief description of function rank and how it is equivalent to nested loops or nested \textit{map}s	
	and a literature review of other attempts to solve the same or similar problems
	\item Chapter \ref{paraop} proposes two new operators and a new library for future parallel implementations of the J programming language 
		and illustrates how they might be used
    \item Chapter \ref{probs} gives the listing of our selection of example problems, 
		giving briefly for each a description, 
		a discussion of the relevant parallel design patterns, 
		a comparison between example solutions written in J and C with OpenMP,
		and a description of how to extend these solutions to problems in higher dimensions.
    \item Chapter \ref{res} compares the results of both the example C solutions and solutions written in Scala with Parallel-J 
		to the parallel problems listed in Chapter \ref{probs}
    \item Chapter \ref{conc} presents our conclusions of this research and discusses future work. 
\end{itemize}
\noindent
In addition, appendices and a glossary of J functions used throughout the remaining chapters 
are given at the end of this paper, as aids to the reader.

\section{Design Plan}
\label{desp}
The goal of this research had been 
to implement a parallel subset of the programming language J, called \textit{Parallel-J}.
Based on the documentation of the current implementation of J\cite{ioj}, this language subset would require:
\begin{itemize}
	\item a J language lexer and parser
	\item limited memory management
	\item a look-up table for predefined and user functions
	\item a read-evaluate-print-loop (REPL), and
	\item a subset of the J primitives, including: 
	\begin{itemize}
		\item global variable assignment
		\item most array operations (creating, indexing, restructuring)
		\item all of the basic arithmetic and logical operations
		\item a limited selection of higher-ordered functions (composition, conditional, reduction)
		\item J's function composition rules for sequences of functions (\textit{trains}), and
		\item the rank operator and function rank
	\end{itemize}
\end{itemize}
\noindent
The following language features would be excluded:
\begin{itemize}
	\item namespaces (\textit{locales}) and local variable assignment
	\item all existing environment-altering functions (the \textit{foreign} operator, spelled \texttt{!:})
	\item explicit scripts with imperative-style execution and control structure, and
	\item the J primitives which calculate complex algorithms (prime factorization, derivatives of polynomial equations, etc.)
\end{itemize}

The chosen language features allow solutions to the sample problems in as simple and ``idiomatic'' J as possible, 
showing clearly how function rank lets the programmer more easily exploit data parallelism,
without getting distracted by J's more advanced or nuanced features. 
Also to be included was both proposals and implementations of two new operators (parallel rank and parallel insert), 
and an extension to the existing \textit{foreign} operator to allow for changes to the parallel environment. 
The former would allow for the equivalent of parallel map and reduce operations at specified rank(s); 
the latter would allow the programmer to set environmental parameters for parallelizing code, 
for example specifying the number of threads to use or setting the thread scheduling scheme.
Although not entirely related to function rank, programmers would expect such environmental controls 
from any serious parallel computing language or library.

Scala, a programming language developed by Odersky et al.\cite{scala}, 
was chosen to implement this parallel subset of J for the following reasons:
\begin{enumerate} 
	\item Scala supports multiple programming paradigms, such as
	\begin{itemize}
		\item imperative, for the tasks of memory management and object creation,
		\item object oriented, for more structured encapsulation of the rank associated with each function 
			and data associated with arrays, and
		\item functional, to more easily capture J's functional paradigm in the implementation and thus facilitate development
	\end{itemize}
	\item Scala has a feature-rich library for collections, including 
	\begin{itemize}
		\item support for many higher-order operations such as \textit{map} and \textit{reduce}, 
			common to many solutions to data parallel problems\cite{scala28col}, and
		\item a library for parallel collections that exploits the concurrency in data parallel operations\cite{pc},
			which, being similar to the use-cases in this research, promised to make the parallel implementation relatively easy.
	\end{itemize}
\end{enumerate}

The performance of solutions written in Parallel-J to a suite of problems would be compared to
performance of solutions to the same problem written in C with OpenMP, J, and Scala.
Each problem description would also discuss the relative level of abstraction abstraction and scalability 
of the solutions to similar problems in higher dimensions
Parallel-J's performance was not expected to compare favorably against the raw performance of C or the current implementation of J; 
instead, it was hoped that Parallel-J's performance would scale relative to the number of threads.
showing that it does effectively exploit potential concurrency.

\section{Implementation}
\label{imp}
Unfortunately, time constraints did not allow for such an ambitious project.
Rather than finishing a parallel subset of J, the J system libraries developed 
are instead used as a prototype for a Scala regular collections library that supports function rank.
Taking the same set of parallel problems, 
this research shows both how this library allows for uniformly extending these solutions to higher dimensions and 
how with future work it could achieve good performance through parallelization.
Parallel-J's performance is also compared with the performance of solutions written in C with OpenMP, 
along with a discussion of the relative level of abstraction, scalability, and performance of each.

Not all of the work done towards producing a parallel implementation of J 
overlaps with producing a Scala library with function rank. 
The majority of this work consists of the proposals 
for the \textit{parallel rank} and \textit{parallel insert} operators and the parallel environment library, 
given in Chapter \ref{paraop}, 
as well as a partial lexer for the J programming language, listed in Appendix \ref{jlex}. 
This work would be useful for future efforts towards a parallel implementation of J.
